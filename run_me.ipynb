{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8869df-5e68-4313-88cd-a2495d2199ee",
   "metadata": {},
   "source": [
    "Author:\n",
    "    - Name: Srikanth M\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aff241-850a-497f-8ff1-4181507080c6",
   "metadata": {},
   "source": [
    "INTRODUCTION:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf19a8-ccdf-4cf1-a2a4-19f067df06a6",
   "metadata": {},
   "source": [
    "The code is expected to predict whether a given consumable is compatible or not compatible. In the case of compatible prediction, \n",
    "the code is expected to further classify the type of consumable used. If the second prediction is of required confidence, it is further\n",
    "used to retrain the model and save the model in onnx format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a97ac-5ac2-4c9e-8b11-686c235430fc",
   "metadata": {},
   "source": [
    "There are three stages:\n",
    "1. stage I : classifies whether an consumable is compatible or not.\n",
    "2. stage II : classifies the type of consumable\n",
    "3. stage III: retrain the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16823e7-d956-4bab-a8c9-55a883e849de",
   "metadata": {},
   "source": [
    "To install the necessary packages,use the following pip commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bb124-7250-46d8-8761-625fd2ed3dd4",
   "metadata": {},
   "source": [
    "pip install matplotlib pandas numpy tensorflow opencv-python pillow\n",
    "\n",
    "pip install --upgrade tensorflow\n",
    "\n",
    "pip install torch torchvision\n",
    "\n",
    "pip install tensorflow keras onnx tf2onnx\n",
    "\n",
    "pip install onnxruntime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3952c-fe2a-4e3c-85cf-f3fdc4c32170",
   "metadata": {},
   "source": [
    "Import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f8ec4f-8fe2-42a9-8193-60421711f1b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f69d5c-4f88-49cf-8689-b5b231fa4b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srik7\\AppData\\Local\\Temp\\ipykernel_4708\\336752527.py:1: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
      "  import imghdr\n"
     ]
    }
   ],
   "source": [
    "import imghdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397abab7-7600-4150-b7f1-188d1244286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6b414f-9872-4505-b381-ae7ec629dcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\srik7\\anaconda3\\Lib\\site-packages\\tf2onnx\\tf_loader.py:68: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\srik7\\anaconda3\\Lib\\site-packages\\tf2onnx\\tf_loader.py:72: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5bdb964-5a0d-4474-9f2c-dd1bd2b0bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f436fe-9364-451c-be82-32f06bad9a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fffef76-7ae9-415b-a644-48bbbd6df5ee",
   "metadata": {},
   "source": [
    "STAGE I CLASSIFCATION MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f624c4c-0f13-4f1e-a555-7d07fcabc91b",
   "metadata": {},
   "source": [
    "Iterate through the data directory containing multiple classes of images, checking each image file for its validity based on its extension. \n",
    "The data directory contains the data for stage I classification. \n",
    "It leads to a folder containing two folders:\n",
    "1. valid consumable\n",
    "2. invalid consumable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be21f4-aa7f-41b8-aa80-aa16794fc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_exts = ['jpeg', 'jpg', 'bmp', 'png']\n",
    "data_dir_1 = 'newdata\\stage1' # data for stage one classificaton\n",
    "for image_class in os.listdir(data_dir_1):\n",
    "    for image in os.listdir(os.path.join(data_dir_1, image_class)):\n",
    "        image_path = os.path.join(data_dir_1, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_exts:\n",
    "                print(\"Image not in extension list{}\".format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print(\"Issue with image {}\".format(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50acfdba-a23a-4a36-bd89-c374ff01ac78",
   "metadata": {},
   "source": [
    "Prepare the stage I data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89dcaa-2b25-4a9b-949f-aee88e3e8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.utils.image_dataset_from_directory(data_dir_1)\n",
    "data_iterator = iter(data)\n",
    "batch = data_iterator.next()\n",
    "scaled =batch[0]/255\n",
    "data = data.map(lambda x,y : (x/255, y))\n",
    "train_size = int(len(data)*.7)\n",
    "val_size = int(len(data)*.2)+1\n",
    "test_size = int(len(data)*.1)+1\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test =data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b4087-6183-46c7-9e5d-4c143cc8b3af",
   "metadata": {},
   "source": [
    "Stage I model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b42d4-c5ba-4e0b-af15-4f409ec71de9",
   "metadata": {},
   "source": [
    " A simple convolutional neural network (CNN) architecture using Keras (tf.keras) for classifying consumable items based on the provided layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e54d8-f1fa-4e43-850f-dfb6f9f4b37d",
   "metadata": {},
   "source": [
    "Usage:\n",
    "\n",
    "This model structure is suitable for binary classification tasks where the input images are 256x256 pixels with 3 color channels (RGB).\n",
    "\n",
    "Compile and train the model using appropriate optimizer, loss function, and training data to fit your specific classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e7993-0237-404e-b4ad-e3cece44e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validconsumablemodel = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6118b3-6424-4b96-afe6-1aeb45c405be",
   "metadata": {},
   "outputs": [],
   "source": [
    "validconsumablemodel.add(Conv2D(16,(3,3), 1, activation = 'relu', input_shape = (256,256,3)))\n",
    "validconsumablemodel.add(MaxPooling2D())\n",
    "\n",
    "validconsumablemodel.add(Conv2D(32,(3,3), 1, activation = 'relu'))\n",
    "validconsumablemodel.add(MaxPooling2D())\n",
    "\n",
    "validconsumablemodel.add(Conv2D(16,(3,3), 1, activation = 'relu'))\n",
    "validconsumablemodel.add(MaxPooling2D())\n",
    "\n",
    "validconsumablemodel.add(Flatten())\n",
    "\n",
    "validconsumablemodel.add(Dense(256, activation = 'relu'))\n",
    "validconsumablemodel.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f89841-fb0a-4a17-bb25-55ac7845d65d",
   "metadata": {},
   "source": [
    "compile the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df12697-f48d-4739-b00b-dc3897154238",
   "metadata": {},
   "source": [
    "This script sets up a simple CNN architecture for binary classification of images (256x256 pixels with 3 color channels).\n",
    "Compiles the model with Adam optimizer and binary cross-entropy loss.\n",
    "Provides a model summary for review and verification of the architecture and parameter counts before training. Adjust the model architecture and hyperparameters as needed for your specific classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d6ac8-1ade-463c-9db2-07ca16975de8",
   "metadata": {},
   "source": [
    "Usage:\n",
    "\n",
    "Ensure to compile the model after defining its architecture and before training it on your dataset.\n",
    "\n",
    "Use model.fit() to train the compiled model with your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987f643-ea47-4f14-9415-2924e82b3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validconsumablemodel.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n",
    "validconsumablemodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1269e-1147-4de4-a127-5689c332b469",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c514d-7b14-4e48-802b-9767aa4d90cb",
   "metadata": {},
   "source": [
    "This setup enables you to track and visualize your model's training progress and performance using TensorBoard.\n",
    "Adjust the logdir path to a directory where you want to store TensorBoard logs.\n",
    "Use %time to monitor training time directly within Jupyter/IPython notebooks.\n",
    "Usage:\n",
    "\n",
    "Ensure you have TensorFlow installed (pip install tensorflow).\n",
    "Set up the train and val datasets appropriately with features and labels for training and validation.\n",
    "Adjust epochs based on your training needs.\n",
    "View TensorBoard logs by running tensorboard --logdir=log in your terminal and navigating to http://localhost:6006 in your web browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab3a5e-051e-450e-a2e6-47c615c1d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='log' #log directory\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logdir)\n",
    "\n",
    "hist = validconsumablemodel.fit(train, epochs = 23, validation_data=val, callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088e23d-a38e-4396-bf20-e374da920360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss from the history object.\n",
    "    \n",
    "    Parameters:\n",
    "    - history (dict): A dictionary containing training and validation loss values.\n",
    "                      Typically obtained from a training process where metrics\n",
    "                      like loss are recorded after each epoch.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Displays a plot using Matplotlib.\n",
    "    \n",
    "    Example usage:\n",
    "    >>> plot_loss(history)\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "      \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee3f76-7dbb-4f9e-8354-01bfa099f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3f6c1-8f54-4950-a892-520ac972c2d6",
   "metadata": {},
   "source": [
    "SAVE THE MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2042f8-7916-42b9-9bb0-f02294b25bdf",
   "metadata": {},
   "source": [
    "The model is saved into two formats:\n",
    "1. h5\n",
    "2. onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb15024-fc11-4706-ad5f-8bb7615ed76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "validconsumablemodel.save(os.path.join('models','stage1validconsumableclassifier.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38a08f-e3b2-440d-a019-588ca19ccbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "validconsumablemodel.output_names=['output']\n",
    "# Convert the Keras model to ONNX format\n",
    "spec = (tf.TensorSpec((None, 256, 256, 3), tf.float32, name=\"input\"),)\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(validconsumablemodel, input_signature=spec, opset=13)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "validconsumablemodel.output_names=['output']\n",
    "# Save the ONNX model to a file\n",
    "onnx_save_path = 'models/stage1validconsumableclassifier.onnx'\n",
    "with open(onnx_save_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"Model has been converted to ONNX format and saved as '{onnx_save_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e987c7e-e743-4c9e-a058-ae2cb0021792",
   "metadata": {},
   "source": [
    "LOAD THE MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df1a9c-b7ac-4d93-91ea-63f6fdc3256d",
   "metadata": {},
   "source": [
    "keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d2c056-045d-4a3e-8fbc-cbe6ea57b218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14400</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,686,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m4,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14400\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m3,686,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,696,627</span> (14.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,696,627\u001b[0m (14.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,696,625</span> (14.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,696,625\u001b[0m (14.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_path = 'models\\stage1validconsumableclassifier.h5' #path\n",
    "\n",
    "# Load the model\n",
    "validconsumablemodel_loaded = load_model(model_path)\n",
    "\n",
    "# Optional: Print model summary\n",
    "validconsumablemodel_loaded.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33ad23-928c-4477-a058-0c4d01a986e9",
   "metadata": {},
   "source": [
    "onnx model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0616be-6242-4b9a-9e77-a02f674a7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = 'models/stage1validconsumableclassifier.onnx'  # Replace with your ONNX model path\n",
    "session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Check the input names of the ONNX model (optional)\n",
    "input_name = session.get_inputs()[0].name\n",
    "print(f'Input name: {input_name}')\n",
    "\n",
    "# Check the output names of the ONNX model (optional)\n",
    "output_name = session.get_outputs()[0].name\n",
    "print(f'Output name: {output_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c753fa16-d9f0-4892-860b-7852a164a4f0",
   "metadata": {},
   "source": [
    "STAGE II CLASSIFICATION MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa34ab-b772-4835-bb44-d51130b126b6",
   "metadata": {},
   "source": [
    "(skip this part if you have enough data)\n",
    "Define a function,\"duplicate_image\", which take in an image, output folder path and number of samples.\n",
    "The copies vary in axis, a 7 degree magnitude of rotation and brightness.\n",
    "The samples are stored in specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7620dc4-01d5-4454-a23c-61b6d23ad922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_image(image_path, output_folder, num_copies=100):\n",
    "    \"\"\"\n",
    "    Duplicate an image multiple times with random transformations and save the augmented images to a specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image file.\n",
    "    - output_folder (str): Path to the folder where augmented images will be saved.\n",
    "    - num_copies (int, optional): Number of augmented copies to create (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Notes:\n",
    "    - If the specified output_folder does not exist, it will be created.\n",
    "    - Each duplicated image will be saved with a unique filename in the format \"augmented_image_i.jpg\".\n",
    "\n",
    "    Example:\n",
    "    >>> duplicate_image('input_image.jpg', 'augmented_images', num_copies=50)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=7),\n",
    "        transforms.ColorJitter(brightness=(0.5, 1.5))\n",
    "    ])\n",
    "\n",
    "    for i in range(num_copies):\n",
    "        transformed_image = transform(image)\n",
    "        transformed_image.save(os.path.join(output_folder, f\"augmented_image_{i + 1}.jpg\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e94b54-f875-4700-93f1-0100281e1599",
   "metadata": {},
   "source": [
    "Data directoy and output class names are defined.\n",
    "\n",
    "Data expected:\n",
    "* compatible consumables divided into their corresponding folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbeb8c-155a-4d4c-a709-fb6cd58759d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'newdata\\stage2' #path\n",
    "CLASS_NAMES= ['130A','50A','90A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba373a-4eb4-4cc4-adb6-c3310dd760b8",
   "metadata": {},
   "source": [
    "TRANSFORMS:\n",
    "\n",
    "Two transforms are created. One for training dataset and one for testing dataset.\n",
    "The transform used for training dataset, resizes, flips, rotates and converts the image to a tensor.\n",
    "the tranform used for testing dataset resizes and converts it to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04a2da-7056-4c9d-84c1-f1bb4dd7b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                          transforms.RandomVerticalFlip(),\n",
    "                                          transforms.RandomRotation(7),\n",
    "                                          transforms.ToTensor()\n",
    "                                          ])\n",
    "testing_transforms = transforms.Compose([transforms.Resize((256,256)),                                                                                  \n",
    "                                         transforms.ToTensor()\n",
    "                                          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455baf3-192d-4be9-8841-c43a8d2f37a5",
   "metadata": {},
   "source": [
    "DATASET CREATION:\n",
    "\n",
    "Two datasets, are created and transformed with their corresponding transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a74c5-e8ac-434b-9122-a5cf23414a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root = data_dir, transform = training_transforms )\n",
    "test_dataset = torchvision.datasets.ImageFolder(root = data_dir, transform = testing_transforms )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7f430-5c55-47a8-b432-d1744ef92b15",
   "metadata": {},
   "source": [
    "These data sets are then fed into a loader, which loads them in specified batches. In this case, it is batches of 23 images.\n",
    "* The train loader shuffles the dataset\n",
    "* the test loader does not shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592a6be-87c7-489b-accd-0a9da57e75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 23, shuffle = True )\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = 23, shuffle = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471de278-e611-4f92-9a32-d950255dcc11",
   "metadata": {},
   "source": [
    "A function, \"show_transformed_images(dataset)\",  to visualize the loader data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ce9dc-5582-49bd-b23e-2229a09dfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transformed_images(dataset):\n",
    "    \"\"\"\n",
    "    Display a grid of transformed images from a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (torch.utils.data.Dataset): Torch dataset object containing images and labels.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Example:\n",
    "    >>> show_transformed_images(my_dataset)\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = 6, shuffle = True)\n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch\n",
    "\n",
    "\n",
    "    grid = torchvision.utils.make_grid(images, nrow = 3)\n",
    "    plt.figure(figsize = (11,12))\n",
    "    plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "    print('labels: ', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbcc340-7ed8-4f42-aa3e-6d173e1c093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_transformed_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952955da-d0b3-4a27-b9d1-bc42472d3148",
   "metadata": {},
   "source": [
    "The function set_device() checks if a CUDA-enabled GPU is available on your system. If CUDA is available, it returns a torch.device object representing the GPU ('cuda:0'), specifically the first GPU if there are multiple. If CUDA is not available (i.e., no GPU is available or CUDA drivers are not properly configured), it returns a torch.device object representing the CPU ('cpu')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3337929b-6208-4528-8ee7-6c3e64111d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set the device for PyTorch computations based on GPU availability.\n",
    "\n",
    "    Returns:\n",
    "    - torch.device: CUDA device ('cuda:0') if GPU is available, otherwise CPU device ('cpu').\n",
    "\n",
    "    Example:\n",
    "    >>> device = set_device()\n",
    "    >>> print(device)\n",
    "    cuda:0  # Example output when GPU is available\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        dev = 'cuda:0'\n",
    "    else:\n",
    "        dev = 'cpu'\n",
    "\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef7204-c895-4e97-aa88-cfee15cc624e",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6be051-37f5-41c1-972e-94f95236d7a0",
   "metadata": {},
   "source": [
    "\n",
    "This train_nn function is designed to train a neural network model using PyTorch, iterating over a specified number of epochs (n_epochs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249329e-ed4a-49f0-8b06-1be26d9abad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_loader, test_loader, criterion, optimizer, n_epochs):\n",
    "    \"\"\"\n",
    "    Train a neural network model using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The neural network model to be trained.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n",
    "    - criterion: Loss function to optimize the model.\n",
    "    - optimizer: Optimization algorithm to update the model parameters.\n",
    "    - n_epochs (int): Number of epochs (iterations over the entire dataset) for training.\n",
    "\n",
    "    Returns:\n",
    "    - torch.nn.Module: Trained neural network model.\n",
    "\n",
    "    Notes:\n",
    "    - The function prints training progress and evaluation results after each epoch.\n",
    "    - It evaluates the model's performance on the test set using a separate function `evaluate_model_on_test_set`.\n",
    "\n",
    "    Example:\n",
    "    >>> trained_model = train_nn(model, train_loader, test_loader, criterion, optimizer, n_epochs=10)\n",
    "    \"\"\"\n",
    "    device = set_device()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch number %d' %(epoch +1))\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        total = 0\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total += labels.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels==predicted).sum().item()\n",
    "        epoch_loss = running_loss/len(train_loader)\n",
    "        epoch_acc = 100.00*running_correct/total\n",
    "        print(\"-----Training dataset, identified %d out of %d images correctly (%.3f%%). epoch loss: %.3f\" % (running_correct, total, epoch_acc, epoch_loss))\n",
    "        evaluate_model_on_test_set(model, test_loader) #Calls a function (evaluate_model_on_test_set) to evaluate the model's performance on the test set after each epoch.\n",
    "    print('finished')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3533e-0e15-43e2-bb54-7b54c106c7b5",
   "metadata": {},
   "source": [
    "\n",
    "The evaluate_model_on_test_set function is designed to evaluate a trained neural network model using a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0302e-d29b-4f7b-98e1-6ea3c6702fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test_set(model, test_loader):\n",
    "    model.eval()\n",
    "    predicted_correctly_on_epoch = 0\n",
    "    total = 0\n",
    "    device = set_device()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total += labels.size(0)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted_correctly_on_epoch += (predicted == labels).sum().item()\n",
    "    \n",
    "            epoch_acc = 100.00 * predicted_correctly_on_epoch/total\n",
    "           \n",
    "            print(' --------testing dataset, identified %d out of %d images correctly (%.3f%%)' %(predicted_correctly_on_epoch, total, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c9a91-224e-4e59-baf7-4ec5484acc70",
   "metadata": {},
   "source": [
    "set up a neural network for image classification using the ResNet-18 architecture pretrained on ImageNet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da3007-2d74-4415-afb8-d67ddae6bfc0",
   "metadata": {},
   "source": [
    "This code snippet initializes a ResNet18 model pre-trained on ImageNet (consumable_classifier) and modifies its fully connected (FC) layer for a custom classification task with 3 classes. It then moves the model to the appropriate device (GPU or CPU), sets up a cross-entropy loss function, and defines an Adam optimizer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091775a-0c91-4f3d-a02b-b5e443be6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumable_classifier  = models.resnet18(pretrained = True)\n",
    "num_features = consumable_classifier.fc.in_features\n",
    "number_of_classes = 3\n",
    "consumable_classifier.fc = torch.nn.Linear(num_features, number_of_classes)\n",
    "\n",
    "device = set_device()\n",
    "consumable_classifier = consumable_classifier.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(consumable_classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25946d2a-5b14-454b-bc10-a11743fedb66",
   "metadata": {},
   "source": [
    "Train the model for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143eabe-b3be-47e0-a38a-da1b495f55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn(consumable_classifier, train_loader, test_loader, loss_function, optimizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8db76-f252-4c63-9aa3-fb2e5a8b3d0a",
   "metadata": {},
   "source": [
    "Set up an image transform to test unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0f0f3b0-d886-46b9-be9d-f8c4c58aabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                         transforms.ToTensor()\n",
    "                                          ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbf11d-1f64-4777-9e45-e4f43d15d021",
   "metadata": {},
   "source": [
    "Function,\"classify(model,image_transforms,image_path,classes)\", takes a single image and classifys it accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "278b7d01-9875-4352-9762-a97a0dcfae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model,image_transforms,image_path,classes):\n",
    "    \"\"\"\n",
    "    Classify an image using a PyTorch model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The trained neural network model to perform inference.\n",
    "    - image_transforms (torchvision.transforms.Compose): Image transformations to be applied.\n",
    "    - image_path (str): Path to the input image file.\n",
    "    - classes (list): List of class labels corresponding to the model's output.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Prints:\n",
    "    - str: Predicted class label based on the highest model output probability.\n",
    "\n",
    "    Example:\n",
    "    >>> from torchvision import transforms\n",
    "    >>> model = ...  # Load your trained model\n",
    "    >>> image_transforms = transforms.Compose([\n",
    "    >>>     transforms.Resize((256, 256)),\n",
    "    >>>     transforms.ToTensor(),\n",
    "    >>> ])\n",
    "    >>> image_path = 'path_to_your_image.jpg'\n",
    "    >>> classes = ['class1', 'class2', 'class3']\n",
    "    >>> classify(model, image_transforms, image_path, classes)\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.eval()\n",
    "    image = Image.open(image_path)\n",
    "    image = image_transforms(image).float()\n",
    "    image = torch.unsqueeze(image,0)\n",
    "    output = model(image)\n",
    "    _,prediction =torch.max(output.data,1)\n",
    "    print(classes[prediction.item()])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fa49d-35f3-4d22-a2fc-f88f1306fa5c",
   "metadata": {},
   "source": [
    "SAVE THE MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ea017-fe6f-4f7e-a548-c6ac9879c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(consumable_classifier.state_dict(), os.path.join('models',\"consumable_classifier_stage2.pth.tar\")) #add the path name and model name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948cf65-21f4-4800-9071-d260c00a557a",
   "metadata": {},
   "source": [
    "lOAD THE MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2829ca9c-6983-44a2-a7cd-299fc7310657",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'models\\consumable_classifier_stage2.pth.tar' #specify correct path\n",
    "\n",
    "consumable_classifier_model_load = models.resnet18(num_classes=3)  \n",
    "\n",
    "state_dict = torch.load(file_path)\n",
    "\n",
    "\n",
    "consumable_classifier_model_load.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "consumable_classifier_model_load = consumable_classifier_model_load.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7bd5a-0b9e-4306-8421-d1ed333b9765",
   "metadata": {},
   "source": [
    "STAGE III CLASSIFY, RETRAIN AND SAVE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "acf1116d-4904-40c5-a65e-047b082b503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classication(model_a,model_b,image_path):\n",
    "    \"\"\"\n",
    "    Classifies an image using two sequential models and performs additional actions based on the predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - model_a: TensorFlow or Keras model for initial classification.\n",
    "    - model_b: PyTorch model for secondary classification and potential retraining.\n",
    "    - image_path (str): Path to the image file to classify.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints results of initial classification and potential retraining.\n",
    "\n",
    "    Example usage:\n",
    "    >>> model_a = tf.keras.models.load_model('model_a.h5')\n",
    "    >>> model_b = load_torch_model('model_b.pth')\n",
    "    >>> image_path = './image.jpg'\n",
    "    >>> classification(model_a, model_b, image_path)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    resize = tf.image.resize(img,(256,256))\n",
    "    yhat = model_a.predict(np.expand_dims(resize/255,0))\n",
    "    print(yhat)\n",
    "    if yhat>0.7:\n",
    "        print('consumable compatible')\n",
    "        image_transform = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                         transforms.ToTensor()\n",
    "                                          ])\n",
    "        CLASS_NAMES= ['130A','50A','90A']\n",
    "        yhat_2 = classify_stage2(model_b,image_transform,image_path,CLASS_NAMES)\n",
    "        if yhat_2> 0.85:\n",
    "            retrain_single_image(model_b, image_path, CLASS_NAMES, num_epochs=1,save_path=\"updatedmodel.pth.tar\") #specify save path\n",
    "    else:\n",
    "        print('consumable not compatible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bbbae33-408b-43c0-8cff-b0b8c3d11074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_stage2(model,image_transforms,image_path,classes):\n",
    "    \"\"\"\n",
    "    Performs a secondary classification on an image using a PyTorch model and prints the predicted class and probability.\n",
    "\n",
    "    Parameters:\n",
    "    - model: PyTorch model used for secondary classification.\n",
    "    - image_transforms (torchvision.transforms.Compose): Transformations to apply to the image before inference.\n",
    "    - image_path (str): Path to the image file to classify.\n",
    "    - classes (list): List of class labels corresponding to the model's output classes.\n",
    "\n",
    "    Returns:\n",
    "    - float: Probability of the predicted class.\n",
    "\n",
    "    Example usage:\n",
    "    >>> model = load_torch_model('model.pth')\n",
    "    >>> image_transforms = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "    >>> image_path = './image.jpg'\n",
    "    >>> classes = ['cat', 'dog', 'rabbit']\n",
    "    >>> probability = classify_stage2(model, image_transforms, image_path, classes)\n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "    image = Image.open(image_path)\n",
    "    image = image_transforms(image).float()\n",
    "    image = torch.unsqueeze(image,0)\n",
    "    output = model(image)\n",
    "    _,prediction =torch.max(output.data,1)\n",
    "    probabilities = F.softmax(output, dim=1)\n",
    "    print(f'Predicted class: {classes[prediction]}')\n",
    "    print(f'Predicted class probability: {probabilities[0][prediction.item()].item()}')\n",
    "    \n",
    "    return probabilities[0][prediction.item()].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbb6bdb4-6707-4689-aaa4-15fe37dbbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_single_image(model, image_path, classes, num_epochs, save_path):\n",
    "    \"\"\"\n",
    "    Retrains a PyTorch model using a single image, saves the updated model,\n",
    "    and exports it to ONNX format for deployment.\n",
    "\n",
    "    Parameters:\n",
    "    - model: PyTorch model to be retrained.\n",
    "    - image_path (str): Path to the image used for retraining.\n",
    "    - classes (list): List of class labels corresponding to the model's output classes.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "    - save_path (str): Path to save the retrained model's state_dict and ONNX file.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints training progress and saves the retrained model.\n",
    "\n",
    "    Example usage:\n",
    "    >>> model = load_pretrained_model('pretrained_model.pth')\n",
    "    >>> image_path = './image.jpg'\n",
    "    >>> classes = ['cat', 'dog', 'rabbit']\n",
    "    >>> num_epochs = 10\n",
    "    >>> save_path = 'retrained_model.pth.tar'\n",
    "    >>> retrain_single_image(model, image_path, classes, num_epochs, save_path)\n",
    "    \"\"\"\n",
    "    def preprocess_image(image_path):\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),  # Ensure the image is of the correct size\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image = Image.open(image_path)\n",
    "        return preprocess(image).unsqueeze(0)\n",
    "\n",
    "    \n",
    "    def predict_image(model, image_path):\n",
    "        image = preprocess_image(image_path)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            _, predicted_class = torch.max(output, 1)\n",
    "        return predicted_class.item()\n",
    "\n",
    "    \n",
    "    image_tensor = preprocess_image(image_path)\n",
    "\n",
    "    \n",
    "    predicted_class = predict_image(model, image_path)\n",
    "    print(f'Predicted class: {classes[predicted_class]}')\n",
    "\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    label_tensor = torch.tensor([predicted_class], dtype=torch.long).to(device)\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image_tensor)\n",
    "        loss = criterion(outputs, label_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    print(\"Model retrained and updated\")\n",
    "\n",
    "    # Save the PyTorch model\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join('models/updatedmodels', save_path))\n",
    "    print(f'Model saved to {os.path.join(\"models/updatedmodels\", save_path)}')\n",
    "\n",
    "    # Save the model in ONNX format\n",
    "    model.eval()\n",
    "    onnx_save_path = os.path.splitext(save_path)[0] + '.onnx'\n",
    "    dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        os.path.join('models/updatedmodels', onnx_save_path),\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    print(f'Model saved to ONNX format')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a5dfe-457c-4d92-b447-4d0006f9eb50",
   "metadata": {},
   "source": [
    "LOAD THE UPDATED MODEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ec84f-56d5-4cce-b5b2-5da967f18ee8",
   "metadata": {},
   "source": [
    "PyTorch Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97fe0b3c-b032-46d9-81ec-fbf5c3599557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[[0.97969407]]\n",
      "consumable compatible\n",
      "Predicted class: 50A\n",
      "Predicted class probability: 0.9999967813491821\n",
      "Predicted class: 50A\n",
      "Epoch [1/1], Loss: 1.2938257455825806\n",
      "Model retrained and updated\n",
      "Model saved to models/updatedmodels\\updatedmodel.pth.tar\n",
      "Model saved to ONNX format\n"
     ]
    }
   ],
   "source": [
    "classication(validconsumablemodel_loaded,consumable_classifier_model_load,'50A_test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fbca7a19-c929-4443-b2e5-57812c8004a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'models/updatedmodels/updatedmodel.pth.tar'\n",
    "#file path for pytorch model\n",
    "\n",
    "model_d = models.resnet18(num_classes=3)  \n",
    "\n",
    "state_dict_1 = torch.load(file_path)\n",
    "\n",
    "\n",
    "model_d.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_d = model_d.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabee8d-c7e4-4777-92cb-1afaf4df5fd4",
   "metadata": {},
   "source": [
    "onnx model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9215f99-6e22-4c1b-9253-bd06485fc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "onnx_model_path = \"models/updatedmodels/updatedmodels.pth.onnx\" #file path for onnx model\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "#  Create an inference session\n",
    "session = ort.InferenceSession(onnx_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db074d-ab7a-4b57-b0a1-3aa6c4145365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocesses an image for a neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image file.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Preprocessed image as a numpy array with shape (1, 3, 224, 224).\n",
    "\n",
    "    Example:\n",
    "    >>> image_path = 'path_to_your_image.jpg'\n",
    "    >>> preprocessed_image = preprocess_image(image_path)\n",
    "    \"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = preprocess(image)\n",
    "    return image.unsqueeze(0).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc49c5-fbbc-4561-a538-d1b3e2e5779c",
   "metadata": {},
   "source": [
    "example use of the loaded onnx model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55909844-85c3-4008-a0f0-ddfc423674e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path =\"90A_1.jpg\" #image to be classified\n",
    "input_data = preprocess_image(image_path)\n",
    "CLASS_NAMES= ['130A','50A','90A']#output class names\n",
    "# Get the name of the input node\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "\n",
    "outputs = session.run(None, {input_name: input_data})\n",
    "\n",
    "# Get the output\n",
    "output_name = session.get_outputs()[0].name\n",
    "predicted_output = outputs[0]\n",
    "\n",
    "# Post-process the output (e.g., get the predicted class)\n",
    "predicted_class = np.argmax(predicted_output, axis=1)\n",
    "print(f'Predicted class: {CLASS_NAMES[predicted_class[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4c1cc-8d3e-47d1-9e30-7908ffd67fc0",
   "metadata": {},
   "source": [
    "example use of the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "58d717cf-f103-47ab-81d7-3f15f609d6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 50A\n",
      "Predicted class probability: 0.9999958276748657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999958276748657"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classify_stage2(consumable_classifier_model_load,image_transform,\"50A_test.jpg\",CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "58339ac6-6dea-4c98-85f9-8c8d2b8ee4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 50A\n",
      "Predicted class probability: 0.9999123811721802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999123811721802"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_stage2(model_d,image_transform,\"50A_test.jpg\",CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf3760-8d59-49ba-adb8-6ce615290b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91105d9-c070-4c94-8e20-ac5db718025a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
